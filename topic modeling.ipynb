{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Akhavan_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlvbFweyEnw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLv5KqD6yeW5"
      },
      "source": [
        "#open data file\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/data2.csv',sep = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV7DHO87yeSw"
      },
      "source": [
        "#show number of documents in each year\n",
        "data.groupby(['Year']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to8Q3dqLyeO4"
      },
      "source": [
        "#clean text\n",
        "\n",
        "import re\n",
        "import gensim\n",
        "from gensim.summarization.textcleaner import split_sentences\n",
        "dt = []\n",
        "for text in data[\"Abstract\"]:\n",
        "    xx1 =  split_sentences(text,)\n",
        "    dt.append(xx1[:-1])\n",
        "\n",
        "mydata = []\n",
        "for i in dt:\n",
        "    result = \".\".join(i)\n",
        "    mydata.append(result)\n",
        "\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  \n",
        "        sent = re.sub('\\s+', ' ', sent)  \n",
        "        sent = re.sub(\"\\'\", \"\", sent)  \n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  \n",
        "\n",
        "data_abstracts = mydata\n",
        "data_words = list(sent_to_words(data_abstracts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eOlvzIXyeLw"
      },
      "source": [
        "#do function for delete stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['This study aimed','This study aimed at finding','This study','The student of','For this purpose',\n",
        "'The responses show','The responses','It was also found that','It was also','Although this survey','The paper has',\n",
        "'The paper also','The paper has investigated','The findings also indicated that','The findings also','The finding',  \n",
        "'In addition the','In addition to','the study observed','In addition the study observed','Though the','The user friendliness of',\n",
        "'Accessibility','The purpose of this research is to study the','The purpose of this research','The research','The development of', \n",
        "'This paper aims','This paper aims to','The findings of this research','This paper presents','the Theory of','The method','The advantages of', \n",
        "'The scope of the','This article','We employed','we show','This article examines','This chapter describes','This chapter','we have','Http','www'])\n",
        "\n",
        "data_words = [[word for word in doc if word not in stop_words and len(word) >= 2] for doc in data_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNxA64xPyeIt"
      },
      "source": [
        "#save words of each document\n",
        "import csv\n",
        "with open('/content/drive/My Drive/x.csv','w',newline='',encoding= 'utf-8')as myfile:\n",
        "     wr=csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "     for d in data_words:\n",
        "         wr.writerow(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "554b5BIiyeFK"
      },
      "source": [
        "#make a trigram from words and stemming\n",
        "\n",
        "import gensim\n",
        "from nltk.stem.porter import*\n",
        "import spacy\n",
        "\n",
        "bigram = gensim.models.Phrases(data_words, min_count=3, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "   \n",
        "    nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
        "    nlp.max_length = 10000000 \n",
        "    \n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([PorterStemmer().stem(token.lemma_) for token in doc if token.pos_ in allowed_postags])\n",
        "   \n",
        "    texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evBjK6FXyeAm"
      },
      "source": [
        "#save ready data for input to model\n",
        "\n",
        "import _pickle as pickle\n",
        "with open('/content/drive/My Drive/data_ready.p', 'wb') as f:\n",
        " data = pickle.dump(data_ready,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmh-dNhRyd9B"
      },
      "source": [
        "#load ready data and make corpus from them and change the corpus with tfidf method\n",
        "import _pickle as pickle\n",
        "import gensim.corpora as corpora\n",
        "from gensim import corpora, models\n",
        "\n",
        "with open('/content/drive/My Drive/data_ready.p', 'rb') as f:\n",
        "    data_ready = pickle.load(f)\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "texts = data_ready\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b_N1c3Yyd4U"
      },
      "source": [
        "#evaluate models in defined range\n",
        "\n",
        "from gensim.models import CoherenceModel\n",
        "import numpy as np\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \n",
        "    coherence_values = []\n",
        "    log_perplexity = []\n",
        "    model_list = []\n",
        "    eval_liklihood = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics, \n",
        "                                           random_state=100,\n",
        "                                           alpha='symmetric',\n",
        "                                           eta=.001,\n",
        "                                           iterations=100,\n",
        "                                           workers=4,\n",
        "                                           dtype=np.float64\n",
        "                                           )        \n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model,texts=texts, corpus=corpus, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        log_perplexity.append(model.log_perplexity(corpus))\n",
        "        eval_liklihood.append(model.bound(corpus))\n",
        "        print(num_topics)\n",
        "        \n",
        "    return model_list, coherence_values,log_perplexity,eval_liklihood\n",
        "\n",
        "model_list_tfidf, coherence_values_tfidf,log_perplexity_tfidf,eval_liklihood_tfidf = compute_coherence_values(dictionary=id2word, corpus=corpus_tfidf,texts=data_ready, start=260, limit=290, step=5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skGUhB_yd1h"
      },
      "source": [
        "# save the models\n",
        "\n",
        "i = 260\n",
        "for model in model_list_tfidf:\n",
        " s = '/content/drive/MyDrive/models/model%s'%i\n",
        " model.save(s)\n",
        " i+=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIrCAXsgydyM"
      },
      "source": [
        "#plot coherence of each model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "limit=290; start=260; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values_tfidf)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"coherence_values_tfidf\")\n",
        "plt.legend((\"coherence_values_tfidf\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFY2gfAFydu7"
      },
      "source": [
        "#plot logarithm of preplexity of each model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Show graph\n",
        "limit=290; start=260; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, log_perplexity_tfidf)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"log_perplexity_tfidf\")\n",
        "plt.legend((\"log_perplexity_tfidf\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1RJo4c8ydpg"
      },
      "source": [
        "#plot liklihood for each model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Show graph\n",
        "limit=290; start=260; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, eval_liklihood_tfidf)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"eval_liklihood_tfidf\")\n",
        "plt.legend((\"eval_liklihood_tfidf\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS6Yqi3Pydm8"
      },
      "source": [
        "#print all topics with 15 most revelant words\n",
        "\n",
        "from pprint import pprint\n",
        "#select the model and print the topic\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "optimal_model.show_topics(formatted=False)\n",
        "optimal_model.print_topics(-1,15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Endq1SUydhp"
      },
      "source": [
        "#details of each document\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def format_topics_sentences(ldamodel, corpus, texts):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "    print(len(ldamodel[corpus]))\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "        print(i)\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus_tfidf, texts=data['Abstract'])\n",
        "\n",
        "#Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "#Show\n",
        "df_dominant_topic.to_csv('/content/drive/MyDrive/details of each document.csv', index=False)\n",
        "df_dominant_topic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vBjma8pydYP"
      },
      "source": [
        "#details of each topic\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/details of each document.csv')\n",
        "start = 0.0\n",
        "end = 260.0\n",
        "all_count = dataframe[dataframe['Dominant_Topic'].notnull()].shape[0]\n",
        "ls = []\n",
        "while start < end:\n",
        "  count = dataframe[dataframe['Dominant_Topic'] == start]\n",
        "  ls.append([start, count.shape[0], count.iloc[0]['Topic_Keywords'], round(count.shape[0]/all_count, 4)])\n",
        "  start += 1\n",
        "new_dataframe = pd.DataFrame(ls, columns=['Dominant_topic', 'Number_of_documents', 'Topic_Keywords', 'percentage_of_topic_documents'])\n",
        "new_dataframe.head(260)\n",
        "new_dataframe.to_csv('/content/drive/MyDrive/details_of_topics.csv', index=False)\n",
        "new_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMNYvuDtydP4"
      },
      "source": [
        "#distibution of each document per words\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "doc_lens = [len(d) for d in data_ready]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16,7), dpi=160)\n",
        "plt.hist(doc_lens, bins = 1000, color='navy')\n",
        "plt.text(450, 750, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
        "plt.text(450, 700, \"Median : \" + str(round(np.median(doc_lens))))\n",
        "plt.text(450, 650, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
        "\n",
        "plt.gca().set(xlim=(0, 500), ylabel='Number of Documents', xlabel='Document Word Count')\n",
        "plt.tick_params(size=16)\n",
        "plt.xticks(np.linspace(0,500,9))\n",
        "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
        "plt.savefig(\"/content/drive/MyDrive/figures/dodwc1.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__myit9mB1UC"
      },
      "source": [
        "#number of documents of each topic in year\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_data = pd.read_csv('/content/drive/MyDrive/data2.csv')\n",
        "df_data_1 = pd.read_csv('/content/drive/MyDrive/details of each document.csv')\n",
        "df_data_1 = df_data_1.join(df_data['Year'])\n",
        "df_data_1 = df_data_1[df_data_1['Dominant_Topic'].notnull()]\n",
        "temp_list = []\n",
        "for i in range (260):\n",
        "  ls = [i]\n",
        "  for j in range (2010, 2020):\n",
        "    ls.append(df_data_1[(df_data_1['Year']==j) & (df_data_1['Dominant_Topic'] == float(i))].shape[0])\n",
        "  temp_list.append(ls)\n",
        "new_df = pd.DataFrame(data=temp_list, columns=['topics','2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019'])\n",
        "new_df.to_csv('/content/drive/MyDrive/topics_per_year_count.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mxIeeAIFoOB"
      },
      "source": [
        "#kmeans clustering(if did this don't run again)\n",
        "\n",
        "import _pickle as pickle\n",
        "import gensim.corpora as corpora\n",
        "from gensim.matutils import corpus2csc,corpus2dense\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora, models\n",
        "with open ('/content/drive/MyDrive/data_ready.p','rb')as f:\n",
        "    data_ready = pickle.load(f)\n",
        "\n",
        "\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "texts = data_ready\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "num_docs = id2word.num_docs\n",
        "num_terms = len(id2word.keys())\n",
        "\n",
        "corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms=num_terms, num_docs=num_docs)\n",
        "\n",
        "#save sparse matrix\n",
        "with open('/content/drive/MyDrive/ccorpus_tfidf_sparse.p', 'wb') as f:\n",
        "    data = pickle.dump(corpus_tfidf_sparse,f)\n",
        "\n",
        "\n",
        "#make kmeans model\n",
        "model = KMeans(n_clusters=260)\n",
        "clusters = model.fit_predict(corpus_tfidf_sparse.T)\n",
        "\n",
        "#save the kmeans model\n",
        "with open('/content/drive/MyDrive/k_means', 'wb') as f:\n",
        "    saved_model = pickle.dump(model, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGT8TeTBGyIz"
      },
      "source": [
        "#k-means clustring details\n",
        "\n",
        "import _pickle as pickle\n",
        "from sklearn.cluster import KMeans\n",
        "import csv\n",
        "\n",
        "with open('/content/drive/MyDrive/k_means', 'rb') as f:\n",
        "      kmeans_model = pickle.load(f)\n",
        "      print(kmeans_model.labels_) #clusters for each document\n",
        "file = open('/content/drive/MyDrive/k_means_results.csv', 'w')\n",
        "k_means_result = [[result] for result in kmeans_model.labels_]\n",
        "print(len(k_means_result))\n",
        "write = csv.writer(file)\n",
        "write.writerows(k_means_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvrXJ8ryCMaX"
      },
      "source": [
        "#linear regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "frame = pd.read_csv('/content/drive/MyDrive/topics_per_year_count.csv')\n",
        "\n",
        "topic_label = 10  #replace 10 with a topic you want to have test regression\n",
        "\n",
        "X = frame.iloc[:, 1:]\n",
        "x = list(map(int, list(frame.columns[1:]))) \n",
        "y = list(X.iloc[topic_label]) \n",
        "\n",
        "temp_list = []\n",
        "for i in x:\n",
        "  temp_list.append([i])\n",
        "x = temp_list\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x, y)\n",
        "#print(regressor.predict[[2020]]) #replace 2020 with a year number you want to predict\n",
        "\n",
        "y_predict = regressor.predict(x) \n",
        "plt.figure(figsize=(12,10))\n",
        "plt.plot(x, y, 'co', label='data')\n",
        "# Plotting the fitted prediction line\n",
        "plt.plot(x, y_predict, linewidth=3.0, label='predicted')\n",
        "plt.plot(x, y, linewidth=3.0, label='trend', color='g')\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('number of documents for topic '+str(topic_label), color='g', fontsize=18)\n",
        "plt.xlabel('Year', color='g', fontsize=18)\n",
        "plt.xticks(color = 'y')\n",
        "plt.yticks(color = 'y')\n",
        "plt.savefig('/content/drive/MyDrive/linear_regression_model.png') #for saving uncomment this line\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri06q22qCMX0"
      },
      "source": [
        "#nonlinear regression with sigmoid method\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import curve_fit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(X, Beta_1, Beta_2):\n",
        "    y = 1 / (1 + np.exp(-Beta_1*(X-Beta_2)))\n",
        "    return y\n",
        "\n",
        "\n",
        "frame = pd.read_csv('/content/drive/MyDrive/topics_per_year_count.csv')\n",
        "\n",
        "topic_label = 10 #replace 10 with a topic you want to have test regression\n",
        "\n",
        "X = frame.iloc[:, 1:]\n",
        "x = list(map(int, list(frame.columns[1:]))) \n",
        "y = list(X.iloc[topic_label]) \n",
        "\n",
        "\n",
        "x_max = max(x)\n",
        "y_max = max(y)\n",
        "\n",
        "x = [item / x_max for item in x]\n",
        "y = [item / y_max for item in y]\n",
        "\n",
        "popt, pcov = curve_fit(sigmoid, x, y)\n",
        "\n",
        "#print(sigmoid(2020/x_max, *popt)*y_max) #replace 2020 with a year number you want to predict\n",
        "\n",
        "y_hat = sigmoid(x, *popt)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.plot([int(item*x_max) for item in x], [int(item*y_max) for item in y], 'co', label='data')\n",
        "# Plotting the fitted prediction line\n",
        "plt.plot([int(item*x_max) for item in x], [int(item*y_max) for item in y_hat], linewidth=3.0, label='predicted')\n",
        "plt.plot([int(item*x_max) for item in x], [int(item*y_max) for item in y], linewidth=3.0, label='trend', color='g')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('number of documents for topic '+str(topic_label), color='g', fontsize=18)\n",
        "plt.xlabel('Year', color='g', fontsize=18)\n",
        "plt.xticks(color = 'y')\n",
        "plt.yticks(color = 'y')\n",
        "plt.savefig('/content/drive/MyDrive/nonlinear_regression_model'+str(topic_label)+'.png') #for saving uncomment this line\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EzWisKiCMVG"
      },
      "source": [
        "#information of document term matrix\n",
        "\n",
        "from itertools import chain \n",
        "import _pickle as pickle\n",
        "import csv\n",
        "\n",
        "with open('/content/drive/MyDrive/data_ready.p', 'rb') as f:\n",
        "    data_ready = pickle.load(f)\n",
        "sum_of_empty_blocks = 0\n",
        "for i in data_ready:\n",
        "    sum_of_empty_blocks += (42473 - len(set(i)))\n",
        "\n",
        "flatten_list = list(chain.from_iterable(data_ready))\n",
        "\n",
        "print('Document term matrix information:')\n",
        "print('matrix size: 50995*42473')\n",
        "print('length of longest word:',len(max(flatten_list, key=len)))\n",
        "print('sum of empty blocks: ', sum_of_empty_blocks)\n",
        "print('sum of full blocks:', 50995*42473 - sum_of_empty_blocks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyH1ih0hCMR5"
      },
      "source": [
        "#show world cloud for each topic\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "from pprint import pprint\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['This study aimed','This study aimed at finding','This study','The student of','For this purpose',\n",
        "'The responses show','The responses','It was also found that','It was also','Although this survey','The paper has',\n",
        "'The paper also','The paper has investigated','The findings also indicated that','The findings also','The finding',  \n",
        "'In addition the','In addition to','the study observed','In addition the study observed','Though the','The user friendliness of',\n",
        "'Accessibility','The purpose of this research is to study the','The purpose of this research','The research','The development of', \n",
        "'This paper aims','This paper aims to','The findings of this research','This paper presents','the Theory of','The method','The advantages of', \n",
        "'The scope of the','This article','We employed','we show','This article examines','This chapter describes','This chapter','we have','Http','www'])\n",
        "\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "# print(optimal_model.show_topics(260,10,formatted=True))\n",
        "\n",
        "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = optimal_model.show_topics(260,100,formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(130, 2, figsize=(10,500), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "# plt.savefig('/content/drive/MyDrive/word_cloud.png') #for save uncomment this line\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTNPImTfCMOr"
      },
      "source": [
        "#visualize the data of lda model with corpus\n",
        "\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "from gensim import corpora, models\n",
        "import _pickle as pickle\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "with open('/content/drive/MyDrive/data_ready.p', 'rb') as f:\n",
        "    data_ready = pickle.load(f)\n",
        "\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "texts = data_ready\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(optimal_model, corpus_tfidf, dictionary=optimal_model.id2word)\n",
        "vis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ababLAUDCMLt"
      },
      "source": [
        "#theta values\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "\n",
        "with open('/content/drive/My Drive/data_ready.p', 'rb') as f:\n",
        "    data_ready = pickle.load(f)\n",
        "\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "texts = data_ready\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "df = pd.DataFrame(columns=[j for j in range(260)])\n",
        "for i in range (len(corpus_tfidf)):\n",
        "  x = optimal_model.get_document_topics(corpus_tfidf[i], -1)\n",
        "  z = [r[1] for r in x]\n",
        "  df.loc[i] = z\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/theta_values.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhnIe6TmCMGB"
      },
      "source": [
        "#phi values\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "\n",
        "topics_terms = optimal_model.state.get_lambda()\n",
        "\n",
        "#convert estimates to probability (sum equals to 1 per topic)\n",
        "topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)\n",
        "\n",
        "# find the right word based on column index\n",
        "words = [optimal_model.id2word[i] for i in range(topics_terms_proba.shape[1])]\n",
        "\n",
        "#put everything together\n",
        "fram = pd.DataFrame(topics_terms_proba,columns=words)\n",
        "fram.to_csv('/content/drive/MyDrive/phi_values.csv')\n",
        "fram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WoMC1xKCMA8"
      },
      "source": [
        "#corrolation\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "\n",
        "def plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n",
        "    \"\"\"Helper function to plot difference between models.\n",
        "\n",
        "    Uses matplotlib as the backend.\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots(figsize=(20, 20))\n",
        "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(data)\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "corrolationofmodel , annotation= optimal_model.diff(optimal_model, distance='hellinger', num_words=83)\n",
        "np.savetxt('/content/drive/MyDrive/corrolation.csv', corrolationofmodel)\n",
        "\n",
        "plot_difference_matplotlib(corrolationofmodel, annotation=annotation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ZN0F_VCL6u"
      },
      "source": [
        "#determine hot and cold topics\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import csv\n",
        "\n",
        "df_alpha = pd.read_csv('/content/drive/MyDrive/theta_values.csv', index_col='Unnamed: 0')\n",
        "df_data = pd.read_csv('/content/drive/MyDrive/data2.csv')\n",
        "df_alpha = df_alpha.join(df_data['Year'])\n",
        "x = df_alpha.groupby(['Year']).mean()\n",
        "z = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
        "x['year'] = z\n",
        "z = x['year']\n",
        "x = x.drop(['year'], axis=1)\n",
        "x.to_csv('/content/drive/MyDrive/theta_means.csv')\n",
        "index = [str(i) for i in range(260)]\n",
        "result = []\n",
        "p_values = []\n",
        "for ind in index:  \n",
        "    result.append(stats.linregress(z, x[ind])[0])\n",
        "    p_values.append(stats.linregress(z, x[ind])[3])\n",
        "\n",
        "all_result = [(i, slope) for i, slope in enumerate(result)]\n",
        "p_level = 0.05 #change this p_level by 1-alpha\n",
        "result_slope_neg = [(i, neg) for i, neg in enumerate(result) if neg < 0]\n",
        "result_slope_pos = [(i, pos) for i, pos in enumerate(result) if pos >= 0]\n",
        "total_p_value = [(i, p) for i, p in enumerate(p_values) if p < p_level]\n",
        "significance_pos = [x for x in result_slope_pos for y in total_p_value if y[0]==x[0]]\n",
        "significance_neg = [x for x in result_slope_neg for y in total_p_value if y[0]==x[0]]\n",
        "not_significance = list(set(all_result) - set(significance_pos + significance_neg))\n",
        "print('p-level: '+str(p_level)+' or confidence level: '+str(1-p_level))\n",
        "print('total positive trend: ',len(significance_pos))\n",
        "print('total_negative trend: ', len(significance_neg))\n",
        "print('total neutral:', len(not_significance))\n",
        "print('negative topics:', [(i[0]+1, i[1]) for i in significance_neg])\n",
        "print('positive topics:', [(i[0]+1, i[1]) for i in significance_pos])\n",
        "neutral = [(i[0]+1, i[1]) for i in not_significance]\n",
        "neutral.sort(key=lambda x:x[0])\n",
        "print('neutral topics:', neutral)\n",
        "\n",
        "res = [[i] for i in result]\n",
        "print('\\nall slope results:', res)\n",
        "file = open('/content/drive/MyDrive/all_slope_result.csv','w')\n",
        "with file:     \n",
        "    write = csv.writer(file) \n",
        "    write.writerows(res) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qYHeYk1CLy_"
      },
      "source": [
        "#save negative topics pictures\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "z = [[2010], [2011], [2012], [2013], [2014], [2015], [2016], [2017], [2018], [2019]]\n",
        "\n",
        "\n",
        "\n",
        "f = pd.read_csv('/content/drive/MyDrive/theta_means.csv', index_col=['Year'])\n",
        "\n",
        "for j in [i[0] for i in not_significance]:\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(z, f.T.iloc[j])\n",
        "    y_predict = regressor.predict(z)\n",
        "    plt.figure(figsize=(12,10))\n",
        "    # plt.plot(z, f.T.iloc[j], 'co', label='data')\n",
        "    plt.plot(z, f.T.iloc[j], linewidth=3.0, color='g')\n",
        "    plt.plot(z, y_predict, linewidth=3.0)\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylabel('mean theta '+str(j+1), color='g', fontsize=18)\n",
        "    plt.xlabel('Year', color='g', fontsize=18)\n",
        "    plt.savefig('/content/drive/MyDrive/neutral/neutral_topic_'+str(j+1)+'.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3MsgZArCLrV"
      },
      "source": [
        "#save negative topics pictures\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "z = [[2010], [2011], [2012], [2013], [2014], [2015], [2016], [2017], [2018], [2019]]\n",
        "\n",
        "\n",
        "\n",
        "f = pd.read_csv('/content/drive/MyDrive/theta_means.csv', index_col=['Year'])\n",
        "\n",
        "for j in [i[0] for i in significance_neg]:\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(z, f.T.iloc[j])\n",
        "    y_predict = regressor.predict(z)\n",
        "    plt.figure(figsize=(12,10))\n",
        "    # plt.plot(z, f.T.iloc[j], 'co', label='data')\n",
        "    plt.plot(z, f.T.iloc[j], linewidth=3.0, color='g')\n",
        "    plt.plot(z, y_predict, linewidth=3.0)\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylabel('mean theta '+str(j+1), color='g', fontsize=18)\n",
        "    plt.xlabel('Year', color='g', fontsize=18)\n",
        "    plt.savefig('/content/drive/MyDrive/neg/negative_topic_'+str(j+1)+'.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j5Q2hVQCLbo"
      },
      "source": [
        "#save positive topics pictures\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "z = [[2010], [2011], [2012], [2013], [2014], [2015], [2016], [2017], [2018], [2019]]\n",
        "\n",
        "\n",
        "\n",
        "f = pd.read_csv('/content/drive/MyDrive/theta_means.csv', index_col=['Year'])\n",
        "\n",
        "for j in [i[0] for i in significance_pos]:\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(z, f.T.iloc[j])\n",
        "    y_predict = regressor.predict(z)\n",
        "    plt.figure(figsize=(12,10))\n",
        "    # plt.plot(z, f.T.iloc[j], 'co', label='data')\n",
        "    plt.plot(z, f.T.iloc[j], linewidth=3.0, color='g')\n",
        "    plt.plot(z, y_predict, linewidth=3.0)\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylabel('mean theta '+str(j+1), color='g', fontsize=18)\n",
        "    plt.xlabel('Year', color='g', fontsize=18)\n",
        "    plt.savefig('/content/drive/MyDrive/pos/positive_topic_'+str(j+1)+'.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4KaW9sUHO2m"
      },
      "source": [
        "#top 15 words of each topic with frequency or probablity\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "ls = []\n",
        "for i in range(260):\n",
        "    ls.append(optimal_model.show_topic(i, 15))\n",
        "\n",
        "file = open('/content/drive/MyDrive/top_15_word_with_frequency.csv', 'w') \n",
        "  \n",
        "# writing the data into the file \n",
        "with file:     \n",
        "    write = csv.writer(file) \n",
        "    write.writerows(ls) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drKKElsTHOzZ"
      },
      "source": [
        "#1000 words wordcloud with most probablity\n",
        "\n",
        "import _pickle as pickle\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore, tfidfmodel\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "optimal_model = LdaMulticore.load('/content/drive/MyDrive/models/model260')\n",
        "\n",
        "terms = []\n",
        "\n",
        "for i in range(260):\n",
        "    temp = optimal_model.show_topic(i, 15)\n",
        "    for term in temp:\n",
        "        terms.append(term)\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def terms_to_wordcounts(terms, multiplier=1000):\n",
        "    return  ' '.join([' '.join(int(multiplier*i[1]) * [i[0]]) for i in terms])\n",
        "\n",
        "wc = WordCloud(background_color='white',width=1024, height=764, collocations=False).generate(terms_to_wordcounts(terms))\n",
        "\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"terms1.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwtXuM01HOwX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}